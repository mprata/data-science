{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Clustering\n",
    "\n",
    "_Author: Sinan Uozdemir (San Francisco)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Know the difference between supervised and unsupervised learning.\n",
    "- Understand and know how to apply k-means clustering.\n",
    "- Understand and know how to apply density-based clustering (DBSCAN).\n",
    "- Define the Silhouette Coefficient and how it relates to clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Unsupervised Learning](#unsupervised-learning)\n",
    "\t- [Unsupervised Learning Example: Coin Clustering](#unsupervised-learning-example-coin-clustering)\n",
    "\t- [Common Types of Unsupervised Learning](#common-types-of-unsupervised-learning)\n",
    "\t- [Using Multiple Types of Learning Together](#using-multiple-types-of-learning-together)\n",
    "- [Clustering](#clustering)\n",
    "- [K-Means: Centroid Clustering](#k-means-centroid-clustering)\n",
    "\t- [Visual Demo](#visual-demo)\n",
    "\t- [K-Means Assumptions](#assumptions-are-important-k-means-assumes)\n",
    "- [K-Means Demo](#k-means-demo)\n",
    "\t- [K-Means Clustering](#k-means-clustering)\n",
    "\t- [Repeat With Scaled Data](#repeat-with-scaled-data)\n",
    "- [DBSCAN: Density-Based Clustering](#dbscan-density-based-clustering)\n",
    "\t- [Visual Demo](#visual-demo)\n",
    "- [DBSCAN Clustering Demo](#dbscan-clustering-demo)\n",
    "- [Hierarchical Clustering](#hierarchical-clustering)\n",
    "- [Clustering Metrics](#clustering-metrics)\n",
    "- [Clustering, Classification, and Regression](#clustering-classification-and-regression)\n",
    "- [Comparing Clustering Algorithms](#comparing-clustering-algorithms)\n",
    "- [Lesson Summary](#lesson-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unsupervised-learning\"></a>\n",
    "## Unsupervised Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning focuses on finding a relationship between a matrix of features and a response variable. \n",
    "\n",
    "There is typically additional (latent) structure hiding in the feature matrix. For example, some features might be related to each other or even redundant. There also could be groups of observations that seem to be related.\n",
    "\n",
    "Taking advantage of these latent structures allows us to study data without an explicit response in mind and to find better representations for our data to improve predictive performance.\n",
    "\n",
    "**Unsupervised learning** is designed to identify these kinds of structural relationships in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The primary goal of unsupervised learning is \"representation.\"** Unsupervised learning extracts structure from data. For example, you could segment grocery-store shoppers into \"clusters\" of shoppers who exhibit similar behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have primarily studied supervised algorithms: Each observation (row of data) comes with one or more labels -- either categorical variables (classes) or measurements (regression).\n",
    "\n",
    "Unsupervised learning has a different goal: feature discovery.\n",
    "\n",
    "> One common and fundamental example of unsupervised learning is **clustering**. Clustering algorithms are used to find meaningful groups within data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning is clearly differentiated from supervised learning.** With unsupervised learning:\n",
    "\n",
    "- There's no clear objective.\n",
    "- There's no \"right answer\" (which means it's hard to tell how well you're doing).\n",
    "- There's no response variable — only observations with features.\n",
    "- Labeled data is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unsupervised-learning-example-coin-clustering\"></a>\n",
    "### An Example of Unsupervised Learning: Coin Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observations: Coins\n",
    "- Features: Size and mass\n",
    "- Response: None (no hand-labeling required!)\n",
    "\n",
    "- Perform unsupervised learning:\n",
    "  - Cluster the coins based on “similarity.”\n",
    "  - You’re done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/unsupervised-coin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would you imagine a plot of US coins to look like? Are these coins likely US coins (pennies, nickels, dimes, and quarters)?\n",
    "\n",
    "**Answer:** ---\n",
    "\n",
    "What conclusions could you make about this group of coins?\n",
    "\n",
    "**Answer:** ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"common-types-of-unsupervised-learning\"></a>\n",
    "### Common Types of Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering:** Group “similar” data points together.\n",
    "\n",
    "**Dimensionality Reduction:** Reduce the dimensionality of a data set by extracting features that capture most of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"using-multiple-types-of-learning-together\"></a>\n",
    "### Using Multiple Types of Learning Together\n",
    "None of these techniques are mutually exclusive, and they can be used in combination for better results. A useful example is **transfer learning**.\n",
    "\n",
    "Imagine you have a 100,000-row data set with no response values. Your job is to create a response with hand-labels and then create an algorithm to predict the response using supervised learning. \n",
    "\n",
    "Unfortunately, you only have time to label 10,000 rows. Does that mean the rest of the data is useless?\n",
    "\n",
    "The extra 90,000 rows are very useful! We can first use unsupervised learning to identify hidden structures such as related features and groups that naturally form in our data. The unsupervised learning algorithms can transform both our unlabeled and labeled data into a form that's easier to predict from.\n",
    "\n",
    "We then use the new transformed data with supervised learning to get the predictions we were looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering\"></a>\n",
    "## Clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to cover three major clustering approaches:\n",
    "\n",
    "- **Centroid clustering using k-means:** Looks for the centers of k pre-specified groups.\n",
    "\n",
    "- **Density-based clustering using DBSCAN:** Looks at gaps, or lack thereof, between datapoints.\n",
    "\n",
    "- **Hierarchical clustering using agglomerative clustering:** Forms groups of groups of groups in a hierarchy to determine clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering\n",
    "\n",
    "Similarly to k-nearest neighbors, this partitions the entire space into regions (Voronoi partitions). In k-means clustering, k refers to the number of clusters. Also, since this is unsupervised learning, the regions are determined by the k-means algorithm instead of being provided by the training data.\n",
    "\n",
    "**Question:** Why might data often appear in centered clusters?\n",
    "\n",
    "![](./assets/images/clustering-centroids.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density-Based Clustering\n",
    "\n",
    "In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), clusters are created from areas of high density. This can lead to irregularly shaped regions. Also, many parts of space may not belong to any region.\n",
    "\n",
    "**Question:** Why might data often appear in density-based clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/density-clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering\n",
    "\n",
    "In hierarchical clustering, clusters are composed by joining two smaller clusters together.\n",
    "\n",
    "Below, we see a tree data structure that stores clusters of points:\n",
    "- Each node represents a cluster of one or more data points.\n",
    "- Each leaf represents a single data point.\n",
    "- The root is the cluster containing all data points.\n",
    "- Each parent combines its children's clusters to create a new (larger) cluster.\n",
    "\n",
    "**Question:** When might hierarchical clustering be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/hierarchical-clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How is unsupervised learning different from classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Can you think of real-world clustering applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-means-centroid-clustering\"></a>\n",
    "## K-Means: Centroid Clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is a popular centroid-based clustering algorithm.\n",
    " \n",
    "In k-means clustering, we find $k$ clusters (where $k$ is user-specified), each distributed around a single point (called a **centroid**, an imaginary \"center point\" or the cluster's \"center of mass\").\n",
    "\n",
    "> **K-means seeks to minimize the sum of squares of each point about its cluster centroid.**\n",
    "\n",
    "If we manage to minimize this, then we claim to have found good clusters.\n",
    "\n",
    "- As a class, try drawing three clusters and data points in which this constraint is met.\n",
    "- Draw another diagram where the constraint is not met.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means seeks to minimize the sum of squares of each point about its cluster centroid.\n",
    " \n",
    "Let's see step-by-step how we might write an error function that describes this goal. Luckily, we already have experience with sum of squares, so we'll start there. \n",
    "\n",
    "> Although the final equation might look complex, you can understand it by thinking about it step-by-step and always relating each step back to the original goal. Do not let any fear of math get in your way!\n",
    "\n",
    "Keep in mind that unlike a lot of what you do in math, the math we're about to see is not a \"law\" and it is not \"derived\" from any equation. It was arrived at only because someone thought the goal metric above would be a useful description of one type of cluster. As we showed earlier, many other definitions of \"clusters\" are equally valid.\n",
    "\n",
    "#### Step One: Definitions\n",
    "\n",
    "Let's step through a few definitions we'll need to formalize the statement. Note that every data point must belong to exactly one cluster.\n",
    "\n",
    "- $S_i$ is the set of points in the $i$th cluster. For example:\n",
    "    - $S_1 = \\{(1, 1), (2, 1), (0, 1)\\}$.\n",
    "    - $S_2 = \\{(10, 10), (12, 10)\\}$.\n",
    "\n",
    "\n",
    "- $x$ is an arbitrary point. For example:\n",
    "    - $x = (2, 1)$.\n",
    "    - Note that $x \\in S_1$. This reads: \"$x$ belongs to $S_1$\" or just \"$x$ in $S_1$.\"\n",
    "    \n",
    "\n",
    "- $\\mu_i$ is the centroid (\"center point\") of all points in $S_i$. For example:\n",
    "    - $\\mu_1 = (\\frac{1+2+0}{3}, \\frac{1+1+1}{3}) = (1, 1)$.\n",
    "    - $\\mu_2 = (\\frac{10+12}{2}, \\frac{10+10}{2}) = (11, 10)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Two: Error of one cluster\n",
    "\n",
    "We need to measure the \"tightness\" of each cluster -- the closer its points are to the centroid, the better. So, we'll measure how far away each point is from the centroid. Further, we'll square each distance to particularly penalize far away points.\n",
    "\n",
    "So, the sum of the distances of each point $x$ to $\\mu$ is just:\n",
    "\n",
    "$$E_i(S) = {\\sum_{x \\in S} {\\|x - \\mu\\|^2}}$$\n",
    "\n",
    "> This is read: \"The sum of the square distances of each point in S to the centroid of S.\"\n",
    "\n",
    "**Question:** How does this relate to the goal statement?\n",
    "\n",
    "**Answer:** ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Three: Sum of all cluster errors\n",
    "\n",
    "Now, let's find this sum for each cluster. If we sum these sums together, that is the total error for all $k$ clusters:\n",
    "\n",
    "$$E_{total}(S_1, ..., S_k) = \\sum_{i=1}^k E_i(S_i)$$\n",
    "\n",
    "$$= \\sum_{i=1}^k {\\sum_{x \\in S} {\\|x - \\mu\\|^2}}$$\n",
    "\n",
    "**Question:** How does this relate to the goal statement?\n",
    "\n",
    "**Answer:** ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Four: Find the clusters that minimize total error\n",
    "\n",
    "Precisely, find $k$ partitions $S_1, …, S_k$ of the data with centroids $\\mu_1, …, \\mu_k$ that minimize $E_{total}$. In other words:\n",
    "\n",
    "$$\\text{argmin}_{S_1, …, S_k} \\sum_{i=1}^k {\\sum_{x \\in S_i} {\\|x - \\mu_i\\|^2}}$$\n",
    "\n",
    "> $\\text{argmin}_{S_1, …, S_k}\\ f(S_1, ..., S_k)$: Find the values of $S_1, ..., S_k$ that minimize $f(S_1, ..., S_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a computationally difficult problem to solve, so we often rely on heuristics.\n",
    "\n",
    "The \"standard\" heuristic is called **Lloyd’s Algorithm**:\n",
    "1. Start with $k$ initial (random) points* (we'll call these \"centroids\").\n",
    "2. Assign each datapoint to a cluster by finding its \"closest\" centroid (e.g. using Euclidean distance).\n",
    "3. Calculate new centroids based on the datapoints assigned to each cluster.\n",
    "4. Repeat 2-4 until clusters do not change.\n",
    "\n",
    "\\* There are a number of techniques for choosing initial points. For example, see the `k-means++` technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visual-demo\"></a>\n",
    "### Visual Demo\n",
    "\n",
    "[Click through](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) for a demo of k-means clustering in action.\n",
    "\n",
    "![voronoi](assets/voronoi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"assumptions-are-important-k-means-assumes\"></a>\n",
    "### K-Means Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means assumes:\n",
    "\n",
    "- k is the correct number of clusters.\n",
    "- The data is isotropically distributed (circular/spherical distribution).\n",
    "- The variance is the same for each variable.\n",
    "- Clusters are roughly the same size.\n",
    "\n",
    "View these resources to see counterexamples/cases where assumptions are not met:\n",
    "- [Variance Explained](http://varianceexplained.org/r/kmeans-free-lunch/)\n",
    "- [Scikit-Learn](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we choose k?\n",
    "\n",
    "Finding the correct k to use for k-means clustering is not a simple task.\n",
    "\n",
    "We do not have a ground-truth we can use, so there isn't necessarily a \"correct\" number of clusters. However, we can find metrics that try to quantify the quality of our groupings.\n",
    "\n",
    "Our application is also an important consideration. For example, during customer segmentation we want clusters that are large enough to be targetable by the marketing team. In that case, even if the most natural-looking clusters are small, we may try to group several of them together so that it makes financial sense to target those groups.\n",
    "\n",
    "**Common approaches include:**\n",
    "- Figuring out the correct number of clusters from previous experience.\n",
    "- Using the elbow method to find a number of clusters that no longer seems to improve a clustering metric by a noticeable degree.\n",
    "  - The silhouette coefficient is a commonly used measure.\n",
    "  - For an example, check out this [silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) documentation on sklearn.\n",
    "  - If we're using clustering to improve performance on a supervised learning problem, then we can use our usual methods to test predictions.\n",
    "  \n",
    "**It's tempting to \"tune\" k as we have in supervised learning:**\n",
    "  - If we are working on a supervised learning problem, then this is possible.\n",
    "  - If we are using clustering to explore our data, then tuning is of little benefit since we do not know precisely what we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-means-demo\"></a>\n",
    "## K-Means Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>calories</th>\n",
       "      <th>sodium</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Budweiser</td>\n",
       "      <td>144</td>\n",
       "      <td>15</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schlitz</td>\n",
       "      <td>151</td>\n",
       "      <td>19</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lowenbrau</td>\n",
       "      <td>157</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kronenbourg</td>\n",
       "      <td>170</td>\n",
       "      <td>7</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heineken</td>\n",
       "      <td>152</td>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Old_Milwaukee</td>\n",
       "      <td>145</td>\n",
       "      <td>23</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Augsberger</td>\n",
       "      <td>175</td>\n",
       "      <td>24</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Srohs_Bohemian_Style</td>\n",
       "      <td>149</td>\n",
       "      <td>27</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Miller_Lite</td>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Budweiser_Light</td>\n",
       "      <td>113</td>\n",
       "      <td>8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Coors</td>\n",
       "      <td>140</td>\n",
       "      <td>18</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Coors_Light</td>\n",
       "      <td>102</td>\n",
       "      <td>15</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Michelob_Light</td>\n",
       "      <td>135</td>\n",
       "      <td>11</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Becks</td>\n",
       "      <td>150</td>\n",
       "      <td>19</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kirin</td>\n",
       "      <td>149</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pabst_Extra_Light</td>\n",
       "      <td>68</td>\n",
       "      <td>15</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hamms</td>\n",
       "      <td>139</td>\n",
       "      <td>19</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Heilemans_Old_Style</td>\n",
       "      <td>144</td>\n",
       "      <td>24</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Olympia_Goled_Light</td>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Schlitz_Light</td>\n",
       "      <td>97</td>\n",
       "      <td>7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  calories  sodium  alcohol  cost\n",
       "0              Budweiser       144      15      4.7  0.43\n",
       "1                Schlitz       151      19      4.9  0.43\n",
       "2              Lowenbrau       157      15      0.9  0.48\n",
       "3            Kronenbourg       170       7      5.2  0.73\n",
       "4               Heineken       152      11      5.0  0.77\n",
       "5          Old_Milwaukee       145      23      4.6  0.28\n",
       "6             Augsberger       175      24      5.5  0.40\n",
       "7   Srohs_Bohemian_Style       149      27      4.7  0.42\n",
       "8            Miller_Lite        99      10      4.3  0.43\n",
       "9        Budweiser_Light       113       8      3.7  0.40\n",
       "10                 Coors       140      18      4.6  0.44\n",
       "11           Coors_Light       102      15      4.1  0.46\n",
       "12        Michelob_Light       135      11      4.2  0.50\n",
       "13                 Becks       150      19      4.7  0.76\n",
       "14                 Kirin       149       6      5.0  0.79\n",
       "15     Pabst_Extra_Light        68      15      2.3  0.38\n",
       "16                 Hamms       139      19      4.4  0.43\n",
       "17   Heilemans_Old_Style       144      24      4.9  0.43\n",
       "18   Olympia_Goled_Light        72       6      2.9  0.46\n",
       "19         Schlitz_Light        97       7      4.2  0.47"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beer data set\n",
    "import pandas as pd\n",
    "url = './data/beer.txt'\n",
    "beer = pd.read_csv(url, sep=' ')\n",
    "beer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How would you cluster these beers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define X.\n",
    "X = beer.drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happened to Y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-means-clustering\"></a>\n",
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=1, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=3, random_state=1)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 0, 0, 2, 1], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cluster labels and sort by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>calories</th>\n",
       "      <th>sodium</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>cost</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Budweiser</td>\n",
       "      <td>144</td>\n",
       "      <td>15</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schlitz</td>\n",
       "      <td>151</td>\n",
       "      <td>19</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lowenbrau</td>\n",
       "      <td>157</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kronenbourg</td>\n",
       "      <td>170</td>\n",
       "      <td>7</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heineken</td>\n",
       "      <td>152</td>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Old_Milwaukee</td>\n",
       "      <td>145</td>\n",
       "      <td>23</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Augsberger</td>\n",
       "      <td>175</td>\n",
       "      <td>24</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Srohs_Bohemian_Style</td>\n",
       "      <td>149</td>\n",
       "      <td>27</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Heilemans_Old_Style</td>\n",
       "      <td>144</td>\n",
       "      <td>24</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hamms</td>\n",
       "      <td>139</td>\n",
       "      <td>19</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Coors</td>\n",
       "      <td>140</td>\n",
       "      <td>18</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kirin</td>\n",
       "      <td>149</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Michelob_Light</td>\n",
       "      <td>135</td>\n",
       "      <td>11</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Becks</td>\n",
       "      <td>150</td>\n",
       "      <td>19</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Budweiser_Light</td>\n",
       "      <td>113</td>\n",
       "      <td>8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Miller_Lite</td>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Coors_Light</td>\n",
       "      <td>102</td>\n",
       "      <td>15</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Schlitz_Light</td>\n",
       "      <td>97</td>\n",
       "      <td>7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pabst_Extra_Light</td>\n",
       "      <td>68</td>\n",
       "      <td>15</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Olympia_Goled_Light</td>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  calories  sodium  alcohol  cost  cluster\n",
       "0              Budweiser       144      15      4.7  0.43        0\n",
       "1                Schlitz       151      19      4.9  0.43        0\n",
       "2              Lowenbrau       157      15      0.9  0.48        0\n",
       "3            Kronenbourg       170       7      5.2  0.73        0\n",
       "4               Heineken       152      11      5.0  0.77        0\n",
       "5          Old_Milwaukee       145      23      4.6  0.28        0\n",
       "6             Augsberger       175      24      5.5  0.40        0\n",
       "7   Srohs_Bohemian_Style       149      27      4.7  0.42        0\n",
       "17   Heilemans_Old_Style       144      24      4.9  0.43        0\n",
       "16                 Hamms       139      19      4.4  0.43        0\n",
       "10                 Coors       140      18      4.6  0.44        0\n",
       "14                 Kirin       149       6      5.0  0.79        0\n",
       "12        Michelob_Light       135      11      4.2  0.50        0\n",
       "13                 Becks       150      19      4.7  0.76        0\n",
       "9        Budweiser_Light       113       8      3.7  0.40        1\n",
       "8            Miller_Lite        99      10      4.3  0.43        1\n",
       "11           Coors_Light       102      15      4.1  0.46        1\n",
       "19         Schlitz_Light        97       7      4.2  0.47        1\n",
       "15     Pabst_Extra_Light        68      15      2.3  0.38        2\n",
       "18   Olympia_Goled_Light        72       6      2.9  0.46        2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer['cluster'] = km.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do the clusters seem to be based on? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 150.        ,   17.        ,    4.52142857,    0.52071429],\n",
       "       [ 102.75      ,   10.        ,    4.075     ,    0.44      ],\n",
       "       [  70.        ,   10.5       ,    2.6       ,    0.42      ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the mean of each feature for each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the `DataFrame` of cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers = beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allow plots to appear in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a \"colors\" array for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'NumPy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dacf1d697865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mNumPy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yellow'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'NumPy'"
     ]
    }
   ],
   "source": [
    "import NumPy as np\n",
    "colors = np.array(['red', 'green', 'blue', 'yellow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot of calories versus alcohol, colored by cluster (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(beer.calories, beer.alcohol, c=colors[beer.cluster], s=50);\n",
    "\n",
    "# Cluster centers, marked by \"+\"\n",
    "plt.scatter(centers.calories, centers.alcohol, linewidths=3, marker='+', s=300, c='black');\n",
    "\n",
    "# Add labels.\n",
    "plt.xlabel('calories')\n",
    "plt.ylabel('alcohol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot matrix (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"repeat-with-scaled-data\"></a>\n",
    "### Repeat With Scaled Data\n",
    "\n",
    "Unscaled features cause most algorithms to put too much weight onto one feature. We can scale our data to make sure k-means accounts for all features.\n",
    "\n",
    "Remember that k-means is looking for isotropic groups, meaning that they disperse from the center in all directions evenly. \n",
    "\n",
    "There is more than one choice of scaling method (min/max, z-score, log, etc.), but the best choice is the one that makes your clusters isotropic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Center and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with three clusters on scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3, random_state=1)\n",
    "km.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cluster labels and sort by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beer['cluster'] = km.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the \"characteristics\" of each cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot matrix of new cluster assignments (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you notice any cluster assignments that seem a bit odd? How might we explain those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dbscan-density-based-clustering\"></a>\n",
    "## DBSCAN: Density-Based Clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/dbscan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN: Density-Based Spatial Clustering of Applications With Noise (1996)**\n",
    "\n",
    "The main idea of DBSCAN is to group together closely packed points by identifying:\n",
    "- Core points\n",
    "- Reachable points\n",
    "- Outliers (not reachable)\n",
    "\n",
    "**Its two parameters are:**\n",
    "- `min_samples`: At least this many points are required inside a neighborhood to form a dense cluster.\n",
    "- `eps`: epsion. This is the radius of a neighborhood.\n",
    "\n",
    "**How does it work?** \n",
    "\n",
    "1. Choose a random unvisited data point.\n",
    "2. Find all points in its neighborhood (i.e. at most `eps` units away). Then:\n",
    "    - **If there are at least `min_samples` points in its neighborhood:** Add all points in the neighborhood to the current cluster. Mark them as unvisited if they have not been visited.\n",
    "    - **Otherwise:** Mark the current point as visited. If the point is not part of a cluster, label the point as noise and go to Step 1.\n",
    "3. If another point in the current cluster is unvisited, choose another point in the cluster and go to Step 2. Otherwise, start a new cluster and go to Step 1.\n",
    "\n",
    "As a class, try running the algorithm on the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visual-demo\"></a>\n",
    "### Visual Demo\n",
    "\n",
    "[Click through](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/) for a demo of DBSCAN in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN advantages**:\n",
    "- Can find arbitrarily shaped clusters.\n",
    "- Don’t have to specify number of clusters.\n",
    "- Excludes outliers automatically.\n",
    "\n",
    "**DBSCAN disadvantages**:\n",
    "- Doesn’t work well when clusters are of varying densities.\n",
    "- Hard to choose parameters that work for all clusters.\n",
    "- Can be hard to choose correct parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does DBSCAN differ from k-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dbscan-clustering-demo\"></a>\n",
    "## DBSCAN Clustering Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN with eps=1 and min_samples=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=1, min_samples=3)\n",
    "db.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cluster labels and sort by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beer['cluster'] = db.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot matrix of DBSCAN cluster assignments (0=red, 1=green, 2=blue, -1=yellow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hierarchical-clustering\"></a>\n",
    "## Hierarchical Clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering, like k-means clustering, is another common form of clustering analysis. With this type of clustering we seek to do exactly what the name suggests:\n",
    "\n",
    "- Build hierarchies of clusters.\n",
    "- Connect the clusters in the hierarchy with links.\n",
    "\n",
    "Once the links are determined, we can display them in what is called a dendrogram — a graph that displays all of these links in their hierarchical structure.\n",
    "\n",
    "**As we described earlier:**\n",
    "\n",
    "- Each node represents a cluster of one or more data points.\n",
    "- Each leaf represents a single data point.\n",
    "- The root is the cluster containing all data points.\n",
    "- Each parent combines its children's clusters to create a new (larger) cluster.\n",
    "\n",
    "**A simple algorithm we could use to generate this tree:**\n",
    "\n",
    "1. Create a cluster for each point, containing only that point. (Create all leaf nodes.)\n",
    "2. Choose the two clusters with centroids closest to each other.\n",
    "    - Combine the two clusters into a new cluster that replaces the two individual clusters. (Create a new parent node.)\n",
    "3. Repeat Step 2 until only one cluster remains.\n",
    "\n",
    "As a class, try this algorithm on the board. Draw the graph alongside clustering the data points.\n",
    "\n",
    "Essentially we form groups of groups of groups.\n",
    "\n",
    "![](./assets/images/hierarchical-clustering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg = AgglomerativeClustering(n_clusters=4)\n",
    "agg.fit(X_scaled)\n",
    "labels = agg.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the cluster labels and sort by cluster.\n",
    "beer['cluster'] = agg.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot matrix of DBSCAN cluster assignments (0=red, 1=green, 2=blue, -1=yellow)\n",
    "pd.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering-metrics\"></a>\n",
    "## Clustering Metrics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we need a metric to evaluate model fit.\n",
    " \n",
    "For clustering, we often use a metric called the **Silhouette Coefficient**. There are many other approaches, but this is a good place to start. Keep in mind that \"good fit\" for clustering is often arbitrary. For example, scoring isometetry might not apply if most clusters are naturally arbitrary shapes. \n",
    "\n",
    "The Silhouette Coefficient gives a score for each sample individually. At a high level, it compares the point's cohesion to its cluster against its separation from the nearest other cluster. Ideally, you want the point to be very nearby other points in its own cluster and very far points in the nearest other cluster.\n",
    "\n",
    "Here is how the Silhouette Coefficient is measured. Keep in mind how this math definition compares to our high-level idea of a sample's cohesion vs. separation:\n",
    "\n",
    "$$\\frac {b - a} {max(a,b)}$$\n",
    "\n",
    "- $a$ is the mean distance between a sample and all other points in the cluster.\n",
    "\n",
    "- $b$ is the mean distance between a sample and all other points in the nearest cluster.\n",
    "\n",
    "The coefficient ranges between 1 and -1. The larger the coefficient, the better the clustering.\n",
    "\n",
    "To get a score for all clusters rather than for a particular point, we average over all points to judge the cluster algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.silhouette_score(X, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering-classification-and-regression\"></a>\n",
    "## Clustering, Classification, and Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use clustering to discover new features, then use those features for either classification or regression.\n",
    "\n",
    "For classification, we could use clusters directly to classify new points.\n",
    "\n",
    "For regression, we could use a dummy variable for the clusters as a variable in our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_colors(labels, colors='rgbykcm'):\n",
    "    colored_labels = []\n",
    "    for label in labels:\n",
    "        colored_labels.append(colors[label])\n",
    "    return colored_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create some synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "data = []\n",
    "dist = multivariate_normal(mean=[0, 0], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    p = list(dist.rvs())\n",
    "    data.append(dist.rvs())\n",
    "dist = multivariate_normal(mean=[1, 5], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    data.append(dist.rvs())\n",
    "dist = multivariate_normal(mean=[2, 10], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    data.append(dist.rvs())\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"x\", \"y\"])\n",
    "df.head()\n",
    "plt.scatter(df['x'], df['y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit a DBSCAN estimator.\n",
    "estimator = DBSCAN(eps=0.8, min_samples=10)\n",
    "X = df[[\"x\", \"y\"]]\n",
    "estimator.fit(X)\n",
    "# Clusters are given in the labels_ attribute.\n",
    "labels = estimator.labels_\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add cluster labels back to the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note that -1 clusters are outliers.\n",
    "df[\"cluster\"] = labels\n",
    "df = pd.concat([df, pd.get_dummies(df['cluster'], prefix=\"cluster\")], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a linear model with clusters included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = df[[\"x\", \"cluster_0\", \"cluster_1\", \"cluster_2\"]]\n",
    "y = df['y']\n",
    "model.fit(X, y)\n",
    "\n",
    "print((model.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.scatter(df[\"x\"], model.predict(X), color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if we don't include the clusters we estimated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = df[[\"x\"]]\n",
    "y = df['y']\n",
    "model.fit(X, y)\n",
    "print((model.score(X, y)))\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.scatter(df[\"x\"], model.predict(X), color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-clustering-algorithms\"></a>\n",
    "## Comparing Clustering Algorithms\n",
    "\n",
    "- K-means\n",
    "  - Finds cluster centers.\n",
    "  - Must choose the number of clusters.\n",
    "  - Assumes clusters are isotropic.\n",
    "- DBSCAN\n",
    "  - Inspects local density to find clusters.\n",
    "  - Better than k-means for anisotropic clusters.\n",
    "  - Capable of finding outliers.\n",
    "- Hierarchical clustering\n",
    "  - Finds clusters by forming groups of groups of groups of points.\n",
    "  - Hierarchical clustering works well for non-spherical clusters.\n",
    "  - May be computationally expensive.\n",
    "  - Guaranteed to converge to the same solution (no random initialization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lesson-summary\"></a>\n",
    "## Lesson Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised learning vs. unsupervised learning\n",
    "    - The main difference between the two is whether we use response labels.\n",
    "- K-means, DBSCAN, and hierarchical clustering\n",
    "  - Can you summarize how each algorithm roughly works?\n",
    "- The Silhouette Coefficient\n",
    "  - What does the silhouette coefficient measure?\n",
    "- Using clustering along with supervised learning\n",
    "  - Why would we expect predictive power to improve when we include clusters?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
